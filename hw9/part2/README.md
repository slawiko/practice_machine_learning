### t-SNE MNIST

Данные качал при помощи `sklearn.datasets.fetch_mldata`. Визуализирова только рандомные 10000 элементов из выборки, в угоду скорости. Далее осуществлял перебор параметров: `perplexity` 10, 30, 50, количество итераций 250, 500, 1000, 3000. `learning_rate(epsion)` не перебирал, использовал 500, как в примере на сайте [t-SNE](https://lvdmaaten.github.io/tsne/).

Как понятно из графиков [p=10 iterations=250](./plots/p=10_iterations=250.png), [p=30 iterations=250](./plots/p=30_iterations=250.png), [p=50 iterations=250](./plots/p=50_iterations=250.png) 250 итераций довольно мало. Притом не важно какое perplexity. Не зря это минимальное значение для t-SNE.

Далее рассмотрим каждое значение perplexity отдельно:

#### p=10

При 500 итерациях ![p=10 iterations=500](./plots/p=10_iterations=500.png) уже можно придумать какие-то зависимости. Например `4` и `9` смешались, что можно объяснить тем, что цифры часто похожи. Но больше никаких зависимостей на этой картинке я не увидел.

Здесь: ![p=10 iterations=1000](./plots/p=10_iterations=1000.png) близость `4` и `9` остается, но теперь все кластеры выделились более явно.

При 3000 итераций: ![p=10 iterations=3000](./plots/p=10_iterations=3000.png) все то же самое, только классы разделились ещё сильнее. 

Из интересного, что объяснить не смог: кластеры слегка перемещаются относительно друг друга в зависимости от количества итераций, но не глобально. То есть `5` и `3` всегда рядом, а `8` сначала была ближе к `3`, а потом сместилась и стала одинаково близко к `5` и `3`.

#### p=30

Ситуация с относительным расположением здесь такая же, как была при perplexity = 10. А вот сами кластеры стали более плотными, относительно графиков при perplexity = 10.

Например 500 итераций при p=10 и p=30: ![p=10 iterations=500](./plots/p=10_iterations=500.png) ![p=30 iterations=500](./plots/p=30_iterations=500.png)

И 1000 итераций: ![p=10 iterations=1000](./plots/p=10_iterations=1000.png) ![p=30 iterations=1000](./plots/p=30_iterations=1000.png)

#### p=50

При 500 итерациях ![p=50 iterations=500](./plots/p=50_iterations=500.png) `5` и `3` смешались так же, как и `4` с `9`.

Такая же ситуация при 1000 итераций ![p=50 iterations=1000](./plots/p=50_iterations=1000.png).

В целом большой разницы относительно p = 30 я не заметил. При 3000 итераций кластеры расположены на приблзительно таком же расстоянии, что и при p = 30 (хотя смотреть на расстояние между кластерами ничего толком не говорит, если верить [статье](https://distill.pub/2016/misread-tsne/))

#### Общие

В итоге много закономерностей выделить не удалось, заметил четкую связь между `4` и `9`, `3` `5` и `8`. Количество итераций очевидно должно быть не меньше 500, потому что с бОльшим количеством итераций становилось только лучше. Относительно perplexity сказать четко не могу, такое чувство, что какие-то вещи лучше смотреть на 10, какие-то на 50.
